dataset:
  name: clinc
  # root_path: /home/heyjoonkim/Universal-Domain-Adaptation/data/ # /path/to/dataset/root
  # data_path: /home/pch330/data # /path/to/dataset/root
  model_path: /home/pch330/data/model_data # /path/to/dataset/root
  data_path: /home/heyjoonkim/data/datasets/ # /path/to/dataset/root
  num_common_class: 4
  num_source_class: 4

model:
  model_name_or_path: bert-base-uncased

train:
  # train: False              # train model <-> only test model
  train: True                 # train model <-> only test model
  max_length: 128             # max length for tokenizer
  batch_size: 64       
  num_train_epochs: 2        # total training epoch 
  lr: 1.0e-5
  lr_scheduler_type: linear   # scheduler type
  early_stop: 3               # early stopping epoch
  seed: 1234                  # random seed
test:
  min_threshold: 0.0
  max_threshold: 1.0
  step: 0.005
  threshold: 0.5
  batch_size: 16

log:
  output_dir: /home/pch330/data/universal_domain_adaptation

# Fixed setting
# data_dir: dataset
# save_dir: ckpt
# log_dir: performance
# n_classes : 2 # Changes automatically.
# num_workers: 4
# model_name_or_path: bert-base-uncased
# num_layer: 12 # changes automatically. !!
# max_seq_length: 512
# device: 0
# n_gpu: 1

# Basic config
model_name: "LaCL"
num_train_epochs: 40

# Experiment type (split/non-split)
# split: True
# split_ratio: 0.5 # [0.25, 0.5, 0.75]
# task_name: banking77 # ['clinc150', 'snips', 'banking77', 'mix_snips', 'mix_banking']

# Optimizer & Scheduler
weight_decay: 0.01
adam_epsilon: 1.0e-6

# Data augmentation
beam: 1 # must be 1
span_mask_len: 5


# LaCL config
gp_location: 'token'
gp_pooling: concat
gp_layers: [1,2,3,4,5,6,7,8,9,10,11,12] # global projector layers

cosine_top_k: 1
reg_loss_weight: 0.1 
encoder_dim: 1024 # 1024(encoder dimension)-> 128(projection dimension) in Contrastive learning
projection_dim: 768 # projection dimension in Contrastive learning and hidden dim in Vanilla PLM classification.
temperature: 0.05